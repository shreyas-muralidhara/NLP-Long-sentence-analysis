{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "P1_Sentimental_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaZDEv9SaIAH",
        "colab_type": "text"
      },
      "source": [
        "## P1 Sentimental Analysis\n",
        "**CSC791 - Fall 2020 term**       \n",
        "**Shreyas Muralidhara (schikkb)**\n",
        "\n",
        "Implementing word vectors to classify sentences based on the sentiment they express."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXn1va-vaIAJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "7d01014e-e6f7-4614-a0c5-ee6888aa3433"
      },
      "source": [
        "#Identify the necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xlrd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import re\n",
        "import gensim\n",
        "\n",
        "from gensim.utils import simple_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score,accuracy_score\n",
        "\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer,TfidfTransformer\n",
        "from sklearn.preprocessing import normalize, StandardScaler, RobustScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC,LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bN4WDKyYaIAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import training data into the dataframe\n",
        "input_df = pd.read_excel(\"P1_training.xlsx\", index_column=None,header=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFgrD7AJlkSt",
        "colab_type": "text"
      },
      "source": [
        "### Baseline 1 - Word2Vec\n",
        "#### Step 1 - Tokenize sentences into word tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qo8Mk0RaIAV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "outputId": "9a824afc-ea10-4edd-9e58-120c9c66fc04"
      },
      "source": [
        "token_sentences = []\n",
        "for sentence in input_df['sentence']:\n",
        "    token_sentences.append(list(gensim.utils.simple_tokenize(sentence)))\n",
        "input_df['word_tokens'] = token_sentences\n",
        "\n",
        "#printing label distribution of the input data\n",
        "print('Label distribution for the training dataset:\\n',input_df['label'].value_counts())\n",
        "\n",
        "# Split the data into train and validation\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(input_df['word_tokens'], input_df['label'], test_size=0.2, shuffle = True, stratify = input_df['label'], random_state=0)\n",
        "\n",
        "print(X_train.shape, X_val.shape, Y_train.shape, Y_val.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label distribution for the training dataset:\n",
            " 1    736\n",
            "2    661\n",
            "0    263\n",
            "Name: label, dtype: int64\n",
            "(1328,) (332,) (1328,) (332,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "760eABinOGku",
        "colab_type": "text"
      },
      "source": [
        "#### Step 2 - Compute word vectors for training and validation sets, using word2vec-skip gram, for each word token and average these word vectors to generate the vector for the sentence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7wBG6zhOF70",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "2d803b40-538e-4c5a-8bc5-90efe8640b80"
      },
      "source": [
        "# Compute word vectors for the train dataset\n",
        "w2vmodel = Word2Vec(X_train,size=100,window=5,min_count=4,sg=1)\n",
        "print('Details of the model generated -',w2vmodel)\n",
        "\n",
        "#Average the word vectors generated for sentence vector\n",
        "X_train_vec = []\n",
        "for sent_token in X_train:\n",
        "    #count the words for which the vectors were generated  \n",
        "    ctr = 0\n",
        "    word_token = np.empty((100),int)\n",
        "    for word in sent_token:\n",
        "      if word in w2vmodel.wv.vocab:\n",
        "        word_token = word_token + np.array(w2vmodel[word])\n",
        "        ctr = ctr + 1  \n",
        "    \n",
        "    word_token = word_token/ctr\n",
        "    X_train_vec.append(word_token)\n",
        "    \n",
        "X_train_vec = pd.DataFrame(X_train_vec)\n",
        "print('Shape of training set sentence vectors -',X_train_vec.shape)\n",
        "\n",
        "\n",
        "#Average the word vectors generated for sentence vector\n",
        "X_val_vec = []\n",
        "for sent_token in X_val:\n",
        "    #count the words for which the vectors were generated  \n",
        "    ctr = 0\n",
        "    word_token = np.empty((100),int)\n",
        "    for word in sent_token:\n",
        "      if word in w2vmodel.wv.vocab:\n",
        "        word_token = word_token + np.array(w2vmodel[word])\n",
        "        ctr = ctr + 1  \n",
        "    \n",
        "    word_token = word_token/ctr\n",
        "    X_val_vec.append(word_token)\n",
        "    \n",
        "X_val_vec = pd.DataFrame(X_val_vec)\n",
        "print('Shape of valuidation set sentence vectors -',X_val_vec.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Details of the model generated - Word2Vec(vocab=2188, size=100, alpha=0.025)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Shape of training set sentence vectors - (1328, 100)\n",
            "Shape of valuidation set sentence vectors - (332, 100)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBDxIKwlzKKI",
        "colab_type": "text"
      },
      "source": [
        "#### Step 3 - Train and validate using the classifier for each sentence in 0,1 & 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IDyUBy30fkZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "9c53b949-f814-43b5-dc15-8ba92d93c6f1"
      },
      "source": [
        "# Fit the model for the classifier\n",
        "clf_word2vec = DecisionTreeClassifier().fit(X_train_vec, Y_train)\n",
        "\n",
        "# Predicting the class labels for validation data\n",
        "Y_val_pred = clf_word2vec.predict(X_val_vec)\n",
        "\n",
        "print('Baseline Model 1 - Word2Vec validation metrics:\\nAccuracy -',round(accuracy_score(Y_val,Y_val_pred),4))\n",
        "print('f1 score -', round(f1_score(Y_val,Y_val_pred,labels=None, pos_label=1, average='weighted'),4))\n",
        "print('Classification Report:\\n',classification_report(Y_val, Y_val_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline Model 1 - Word2Vec validation metrics:\n",
            "Accuracy - 0.4759\n",
            "f1 score - 0.4323\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        53\n",
            "           1       0.48      0.67      0.56       147\n",
            "           2       0.48      0.45      0.46       132\n",
            "\n",
            "    accuracy                           0.48       332\n",
            "   macro avg       0.32      0.37      0.34       332\n",
            "weighted avg       0.40      0.48      0.43       332\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzIIOIW3UEgo",
        "colab_type": "text"
      },
      "source": [
        "### Testing the Baseline1 - Word2vec model and storing the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmqRwG7vUeYE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "ae90c9b8-a2c0-4db2-fd87-8f789c15ff54"
      },
      "source": [
        "#importing the test data into the dataframe\n",
        "test_df = pd.read_excel(\"P1_testing.xlsx\", index_column=None,header=0)\n",
        "\n",
        "#tokenize test sentences into word token\n",
        "token_sentences = []\n",
        "for sentence in test_df['sentence']:\n",
        "    token_sentences.append(list(gensim.utils.simple_tokenize(sentence)))\n",
        "\n",
        "#Average the word vectors generated for sentence vector\n",
        "test_vec = []\n",
        "for sent_token in token_sentences:\n",
        "    #count the words for which the vectors were generated  \n",
        "    ctr = 0\n",
        "    word_token = np.empty((100),int)\n",
        "    for word in sent_token:\n",
        "      if word in w2vmodel.wv.vocab:\n",
        "        word_token = word_token + np.array(w2vmodel[word])\n",
        "        ctr = ctr + 1  \n",
        "    \n",
        "    word_token = word_token/ctr\n",
        "    test_vec.append(word_token)\n",
        "    \n",
        "test_vec = pd.DataFrame(test_vec)\n",
        "#print('Shape of test set sentence vectors -',test_vec.shape)\n",
        "\n",
        "#printing label distribution of the input data\n",
        "#print('Label distribution for the test dataset:\\n',input_df['label'].value_counts())\n",
        "\n",
        "# Predicting the class labels for validation data\n",
        "Y_test_pred = clf_word2vec.predict(test_vec)\n",
        "\n",
        "print('Baseline Model 1 - Word2Vec Test metrics:\\nAccuracy -',round(accuracy_score(test_df['label'],Y_test_pred),4))\n",
        "print('f1 score -', round(f1_score(test_df['label'],Y_test_pred,labels=None, pos_label=1, average='weighted'),4))\n",
        "print('Classification Report:\\n',classification_report(test_df['label'], Y_test_pred))\n",
        "\n",
        "test_df = test_df.rename(columns={'label':'gold_label'})\n",
        "test_df['predicted_label'] = Y_test_pred\n",
        "test_df.to_csv('testing_output_word2vec.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Baseline Model 1 - Word2Vec Test metrics:\n",
            "Accuracy - 0.4524\n",
            "f1 score - 0.3335\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        82\n",
            "           1       0.45      0.93      0.60       303\n",
            "           2       0.52      0.09      0.15       298\n",
            "\n",
            "    accuracy                           0.45       683\n",
            "   macro avg       0.32      0.34      0.25       683\n",
            "weighted avg       0.43      0.45      0.33       683\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikerHkuNZDa8",
        "colab_type": "text"
      },
      "source": [
        "### Baseline 2 - Tf-Idf (Term frequency - Inverse Document frequency)\n",
        "#### Step 1 - Convert the collection of sentences to matrix of token counts using Count Vectorizer\n",
        "\n",
        "#### Step 2 - Transform the count matrix into normalized TF or TF-IDF form.\n",
        "\n",
        "#### Step 3 - Train and validate using the classifier for each sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vz4SIuA6a_j1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "c6f78db4-6eab-439c-deeb-3d35887e4526"
      },
      "source": [
        "# Split the sentences into training and validation by stratifying the samples.\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(input_df['sentence'], input_df['label'], test_size=0.2, shuffle = True, stratify = input_df['label'], random_state=0)\n",
        "#print(X_train.shape, X_val.shape, Y_train.value_counts(), Y_val.value_counts())\n",
        "\n",
        "# Step 1 - Generate the token matrix\n",
        "count_vect = CountVectorizer()\n",
        "X_train_counts = count_vect.fit_transform(X_train)\n",
        "X_val_counts = count_vect.transform(X_val)\n",
        "\n",
        "# Step 2 - Transform the count matrix to TF-IDF\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "X_val_tfidf = tfidf_transformer.transform(X_val_counts)\n",
        "\n",
        "# Step 3 - Train and validate the model using Naive Bayes Classifier\n",
        "clf = MultinomialNB().fit(X_train_tfidf, Y_train)\n",
        "Y_val_pred = clf.predict(X_val_tfidf)\n",
        "\n",
        "print('Baseline Model 2 - TF-IDF validation metrics:\\nAccuracy -',round(accuracy_score(Y_val,Y_val_pred),4))\n",
        "print('f1 score -', round(f1_score(Y_val,Y_val_pred,labels=None, pos_label=1, average='weighted'),4))\n",
        "print('Classification Report:\\n',classification_report(Y_val, Y_val_pred))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline Model 2 - TF-IDF validation metrics:\n",
            "Accuracy - 0.5151\n",
            "f1 score - 0.4707\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        53\n",
            "           1       0.54      0.62      0.58       147\n",
            "           2       0.49      0.61      0.54       132\n",
            "\n",
            "    accuracy                           0.52       332\n",
            "   macro avg       0.34      0.41      0.37       332\n",
            "weighted avg       0.43      0.52      0.47       332\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMRFDjT3pLqB",
        "colab_type": "text"
      },
      "source": [
        "### Testing the Baseline 2 - Tf-Idf (Term frequency inverse document frequency) and storing the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08JadsV5pMDg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "bb9d67a2-76f8-484c-e099-1fa4ceedb1ec"
      },
      "source": [
        "#importing the test data into the dataframe\n",
        "test_df = pd.read_excel(\"P1_testing.xlsx\", index_column=None,header=0)\n",
        "\n",
        "X_test_counts = count_vect.transform(test_df['sentence'])\n",
        "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
        "\n",
        "Y_test_pred = clf.predict(X_test_tfidf)\n",
        "\n",
        "print('Baseline Model 2 - TF-IDF Test metrics:\\nAccuracy -',round(accuracy_score(test_df['label'],Y_test_pred),4))\n",
        "print('f1 score -', round(f1_score(test_df['label'],Y_test_pred,labels=None, pos_label=1, average='weighted'),4))\n",
        "print('Classification Report:\\n',classification_report(test_df['label'], Y_test_pred))\n",
        "\n",
        "test_df = test_df.rename(columns={'label':'gold_label'})\n",
        "test_df['predicted_label'] = Y_test_pred\n",
        "test_df.to_csv('testing_output_TF-IDF.csv', index=False)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline Model 2 - TF-IDF Test metrics:\n",
            "Accuracy - 0.5944\n",
            "f1 score - 0.5562\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        82\n",
            "           1       0.59      0.71      0.64       303\n",
            "           2       0.60      0.64      0.62       298\n",
            "\n",
            "    accuracy                           0.59       683\n",
            "   macro avg       0.40      0.45      0.42       683\n",
            "weighted avg       0.52      0.59      0.56       683\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C39M9dT1doU",
        "colab_type": "text"
      },
      "source": [
        "### Proposed Solution - Doc2Vec Model\n",
        "\n",
        "Preprocessing the input data includes\n",
        "  1. Removing Punctuations and replacing by blanks\n",
        "  2. Perform stemming using Porter Stemmer\n",
        "  3. Perform wordnet lemmatozation using nltk.wordnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2u02Mwdg9p7N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perform text Preprocessing - Stemming using Porter Stemmer\n",
        "preprocessed_sentence = []\n",
        "for sentence in input_df['sentence']:\n",
        "    lemma_sentence = []\n",
        "    # Removing punctuations from the sentence\n",
        "    sentence = re.sub(r'[^0-9A-Za-z]+', ' ', sentence)\n",
        "    for word in word_tokenize(sentence):\n",
        "        # Replace the word with stem word \n",
        "        stem_word = PorterStemmer().stem(word)\n",
        "\n",
        "        #Performing lemmmatization on stem words\n",
        "        lemma_sentence.append(WordNetLemmatizer().lemmatize(stem_word))\n",
        "        \n",
        "    preprocessed_sentence.append(lemma_sentence)\n",
        "\n",
        "\n",
        "#Split the input data into Training and validation sets\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(preprocessed_sentence, input_df['label'], test_size=0.2, shuffle = True, stratify = input_df['label'], random_state=0)\n"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gijy-thOjrRB",
        "colab_type": "text"
      },
      "source": [
        "#### Step 1 - Represent each sentence as tagged dcoument containing list of words and their associated tags in it.\n",
        "\n",
        "#### Step 2 - Define the model and build the vocab using training set.\n",
        "\n",
        "#### Step 3 - Generate the document vectors for training and validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgQ-MbycpyNF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "f8fadaec-b605-46b2-db30-ee3ea6382144"
      },
      "source": [
        "# Step 1 - Generating tagged documents with list of words and their associated tags\n",
        "X_train_tagged = [TaggedDocument(d, [i]) for i, d in enumerate(X_train)]\n",
        "X_val_tagged = [TaggedDocument(d, [i]) for i, d in enumerate(X_val)]\n",
        "\n",
        "\n",
        "# Step 2 - Define the model and build the vocab using training set\n",
        "d2vmodel = Doc2Vec(min_count =1,vector_size=100, epochs=20)\n",
        "d2vmodel.build_vocab(X_train_tagged)\n",
        "\n",
        "# Step 3 - Generate the document vectors for training and validation dataset\n",
        "X_train_doc_vectors = []\n",
        "for d in X_train_tagged:  \n",
        "    X_train_doc_vectors.append(d2vmodel.infer_vector(d.words))\n",
        "\n",
        "X_train_doc_vectors = pd.DataFrame(X_train_doc_vectors)\n",
        "\n",
        "X_val_doc_vectors = []\n",
        "for d in X_val_tagged:\n",
        "    X_val_doc_vectors.append(d2vmodel.infer_vector(d.words))\n",
        "\n",
        "X_val_doc_vectors = pd.DataFrame(X_val_doc_vectors)\n",
        "\n",
        "print(X_train_doc_vectors.shape,X_val_doc_vectors.shape)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1328, 100) (332, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuNIoHB0zqlg",
        "colab_type": "text"
      },
      "source": [
        "#### Step 4 - Train and validate the model using the sentence vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r67t2UZ8zo29",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "d8fa6f6a-ca47-4545-815a-4407d44cf7fd"
      },
      "source": [
        "# Fit the model for the classifier\n",
        "clf_word2vec = SVC().fit(X_train_doc_vectors, Y_train)\n",
        "\n",
        "# Predicting the class labels for validation data\n",
        "Y_val_pred = clf_word2vec.predict(X_val_doc_vectors)\n",
        "\n",
        "print('Baseline Model 1 - Word2Vec validation metrics:\\nAccuracy -',round(accuracy_score(Y_val,Y_val_pred),4))\n",
        "print('f1 score -', round(f1_score(Y_val,Y_val_pred,labels=None, pos_label=1, average='weighted'),4))\n",
        "print('Classification Report:\\n',classification_report(Y_val, Y_val_pred))"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline Model 1 - Word2Vec validation metrics:\n",
            "Accuracy - 0.4699\n",
            "f1 score - 0.424\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        53\n",
            "           1       0.48      0.67      0.56       147\n",
            "           2       0.46      0.43      0.45       132\n",
            "\n",
            "    accuracy                           0.47       332\n",
            "   macro avg       0.31      0.37      0.33       332\n",
            "weighted avg       0.39      0.47      0.42       332\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVA0AHP_2Q2T",
        "colab_type": "text"
      },
      "source": [
        "### Testing the Proposed Solution -  Doc2Vec(with Lemmatization and Stemming ) and storing the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXevegqm3O44",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "c48f8d46-ae2b-438e-ca49-62f19d0ace1f"
      },
      "source": [
        "#importing the test data into the dataframe\n",
        "test_df = pd.read_excel(\"P1_testing.xlsx\", index_column=None,header=0)\n",
        "\n",
        "# Perform text Preprocessing - Stemming using Porter Stemmer\n",
        "preprocessed_sentence = []\n",
        "for sentence in test_df['sentence']:\n",
        "    lemma_sentence = []\n",
        "    # Removing punctuations from the sentence\n",
        "    sentence = re.sub(r'[^0-9A-Za-z]+', ' ', sentence)\n",
        "    for word in word_tokenize(sentence):\n",
        "        # Replace the word with stem word \n",
        "        stem_word = PorterStemmer().stem(word)\n",
        "\n",
        "        #Performing lemmmatization on stem words\n",
        "        lemma_sentence.append(WordNetLemmatizer().lemmatize(stem_word))\n",
        "        \n",
        "    preprocessed_sentence.append(lemma_sentence)\n",
        "\n",
        "\n",
        "# Generating tagged documents for test data\n",
        "X_test_tagged = [TaggedDocument(d, [i]) for i, d in enumerate(preprocessed_sentence)]\n",
        "\n",
        "# Generate document vectors for test dataset\n",
        "X_test_doc_vectors = []\n",
        "for d in X_test_tagged:  \n",
        "    X_test_doc_vectors.append(d2vmodel.infer_vector(d.words))\n",
        "\n",
        "X_test_doc_vectors = pd.DataFrame(X_test_doc_vectors)\n",
        "\n",
        "print('Proposed Solution 2 - Doc2Vec Test metrics:\\nAccuracy -',round(accuracy_score(test_df['label'],Y_test_pred),4))\n",
        "print('f1 score -', round(f1_score(test_df['label'],Y_test_pred,labels=None, pos_label=1, average='weighted'),4))\n",
        "print('Classification Report:\\n',classification_report(test_df['label'], Y_test_pred))\n",
        "\n",
        "test_df = test_df.rename(columns={'label':'gold_label'})\n",
        "test_df['predicted_label'] = Y_test_pred\n",
        "test_df.to_csv('testing_output_Doc2Vec.csv', index=False)"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Proposed Solution 2 - Doc2Vec Test metrics:\n",
            "Accuracy - 0.5944\n",
            "f1 score - 0.5562\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        82\n",
            "           1       0.59      0.71      0.64       303\n",
            "           2       0.60      0.64      0.62       298\n",
            "\n",
            "    accuracy                           0.59       683\n",
            "   macro avg       0.40      0.45      0.42       683\n",
            "weighted avg       0.52      0.59      0.56       683\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP5Zf2rgzoS4",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}